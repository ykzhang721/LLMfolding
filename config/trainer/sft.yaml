# logger
logging_strategy: steps
logging_steps: 1
save_strategy: steps
save_steps: 1000
save_total_limit: 5
report_to: wandb
output_dir: dummy
overwrite_output_dir: True

# optimizer
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0
max_steps: 20000
warmup_steps: 1000
learning_rate: 1e-3
lr_scheduler_type: polynomial
lr_scheduler_kwargs:
  lr_end: 1e-4
  power: 1.0

# training
fp16: False
bf16: True
gradient_checkpointing: False
gradient_accumulation_steps: 4
per_device_train_batch_size: 2
packing: True
max_seq_length: 8192
dataset_text_field: text

# evaluation
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 1
